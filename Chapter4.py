# -*- coding: utf-8 -*-
"""MNIST_tesis1.ipynb

Automatically generated by Colaboratory.

"""

import math
import numpy as np
import scipy as sp
import matplotlib.pyplot as plt
import time
import pandas as pd
from sklearn.datasets import fetch_openml
import os
import tempfile
from scipy.optimize import minimize

# This function has partially taken from the program in Louart, Liao, Couillet (2018)
def gen_data(Tr,Te,prop):
    mnist=fetch_openml('mnist_784', cache=False)
    X,y = mnist.data.astype('float32'),mnist.target.astype('int64')
    X_train_full, X_test_full = X[:60000], X[60000:]
    y_train_full, y_test_full = y[:60000], y[60000:]

    #X /=255.0
    
    selected_target = [7,9]
    K=len(selected_target)
    X_train = np.array([]).reshape(p,0)
    X_test = np.array([]).reshape(p,0)        
        
    y_train = []
    y_test = []
    ind=0
    for i in selected_target:
        locate_target_train = np.where(y_train_full==i)[0][range(np.int(prop[ind]*Tr))]
        locate_target_test  = np.where(y_test_full==i)[0][range(np.int(prop[ind]*Te))]
        X_train = np.concatenate( (X_train,X_train_full[locate_target_train].T),axis=1)
        y_train = np.concatenate( (y_train,2*(ind-K/2+.5)*np.ones(np.int(Tr*prop[ind]))) )
        X_test  = np.concatenate( (X_test,X_test_full[locate_target_test].T),axis=1)
        y_test = np.concatenate( (y_test,2*(ind-K/2+.5)*np.ones(np.int(Te*prop[ind]))) )
        ind+=1                       
    
    X_train = X_train - np.mean(X_train,axis=1).reshape(p,1)
    X_train = X_train*np.sqrt(784)/np.sqrt(np.sum(X_train**2,(0,1))/Tr)
    
    X_test = X_test - np.mean(X_test,axis=1).reshape(p,1)
    X_test = X_test*np.sqrt(784)/np.sqrt(np.sum(X_test**2,(0,1))/Te)
        
            
    return X_train, X_test, y_train, y_test
        
n=512     #neurons
p=784     #dimension 
Tr=1024   #number of training points   
Te=Tr     #number of testing points

prop=[.5,.5]      
        

X_train, X_test, Y_train, Y_test = gen_data(Tr,Te,prop)   #data generated

######################### training data ########
T = X_train.shape[1]

######################### testing data ########
h_T = X_test.shape[1]

#### reescalando
Y_train= np.reshape(Y_train,(T,1))
Y_train.shape

Y_test= np.reshape(Y_test,(h_T,1))
Y_test.shape

# transpuestos para dimensiones correctas
Y_train=Y_train.transpose()
Y_test=Y_test.transpose()

#### Entrenamiento (Ajuste de Parametros)
p=X_train.transpose().shape[1]
T=X_train.transpose().shape[0]
n=512   
d=1

#depende de n
W=np.array( [np.random.normal(0,1,p) for e in range(n)] ) ## pesos gausianos n por p

#W.shape

I_n=np.identity(n) 
SIGMA=np.abs( W@X_train )


#### beta estimada
def h_beta(gamma):   # def nombre_de_la_funcion(variable1,...,variablen): enter y definir
    return ( (1/T)*sp.linalg.inv( (1/T)*SIGMA@np.transpose(SIGMA)+gamma*I_n )@SIGMA@np.transpose(Y_train) )

#### error de entrenamiento
def E_train(gamma):
    return ( (1/T)*sp.linalg.norm( np.transpose(Y_train)-np.transpose(SIGMA)@h_beta(gamma) )**2.0 )
    


##### Fase de prueba
h_T=X_test.shape[1]

h_SIGMA=np.abs( W@X_test )

#### error de prueba
def E_test(gamma):
    return ( (1/h_T)*sp.linalg.norm( np.transpose(Y_test)-np.transpose(h_SIGMA)@h_beta(gamma) )**2.0 )

############## hasta aquí ya clasificó y ya corrió la red 
### UNAS GRAFICAS DEL COMPORTAMIENTO DE ERRORES

### prueba
xs = np.linspace(0.1, 1000, 100)
ys = np.zeros_like(xs)

for i in range(len(xs)):
    ys[i] = E_train(xs[i])
    
plt.figure()
plt.plot(xs, ys)
plt.xlabel("gamma")
plt.ylabel("E_train")
plt.show()

#### test
xs = np.linspace(10, 500, 100)
ys = np.zeros_like(xs)

for i in range(len(xs)):
    ys[i] = E_test(xs[i])
    
plt.figure()
plt.plot(xs, ys)
plt.xlabel("gamma")
plt.ylabel("E_test")
plt.show()

###################
### GIC para la parte de prueba
### R,Q no verdaderas s conocido

def Rtest(gamma):
  return( ((1/h_T)*h_SIGMA@np.transpose(h_SIGMA) +gamma*I_n) )

def Qtest(gamma):
  Q_aux=np.array( [[0]*n]*n )
  for i in range(h_T):
    zi=np.reshape(h_SIGMA[:,i],( h_SIGMA.shape[0] ,1))
    yi=np.reshape( Y_test[:,i],(1,1) )
    Q_aux=Q_aux+((zi@np.transpose(zi)@h_beta(gamma)-zi@np.transpose(yi)+gamma*h_beta(gamma))@np.transpose( zi@np.transpose(zi)@h_beta(gamma)-zi@np.transpose(yi) ) )
  return( (1/h_T)*Q_aux )


#### GIC de prueba      
def GICtest(gamma):
  aux=0
  for i in range(h_T):
    zi=np.reshape(h_SIGMA[:,i],( h_SIGMA.shape[0] ,1))
    yi=np.reshape(Y_test[:,i],(1,1) )
    aux=aux+np.transpose(yi-np.transpose(h_beta(gamma))@zi)@(yi-np.transpose(h_beta(gamma))@zi)
  return ( aux+2*np.trace( np.linalg.inv(Rtest(gamma))@Qtest(gamma) ) )

###################
### GIC para la parte de entrenamiento
### R,Q no verdaderas s conocido

def Rtrain(gamma):
  return( ((1/T)* SIGMA@np.transpose(SIGMA) +gamma*I_n) )  

def Qtrain(gamma):
  Q_aux=np.array( [[0]*n]*n )
  for i in range(T):
    zi=np.reshape(SIGMA[:,i],( SIGMA.shape[0] ,1))
    yi=np.reshape( Y_train[:,i],(1,1) )
    Q_aux=Q_aux+((zi@np.transpose(zi)@h_beta(gamma)-zi@np.transpose(yi)+gamma*h_beta(gamma))@np.transpose( zi@np.transpose(zi)@h_beta(gamma)-zi@np.transpose(yi) ) )
  return( (1/T)*Q_aux )
 #### GIC de train
def GICtrain(gamma):
  aux=0
  for i in range(T):
    zi=np.reshape(SIGMA[:,i],( SIGMA.shape[0] ,1))
    yi=np.reshape(Y_train[:,i],(1,1) )
    aux=aux+np.transpose(yi-np.transpose(h_beta(gamma))@zi)@(yi-np.transpose(h_beta(gamma))@zi)
  return ( aux+2*np.trace( np.linalg.inv(Rtrain(gamma))@Qtrain(gamma) ) )

#### Graficando el GIC
# GICtest grafica para ir buscando un buen inicio para la minimizacion
xs = np.linspace(150, 450, 8)
ys = np.zeros_like(xs)

for i in range(len(xs)):
    ys[i] = GICtest(xs[i])
    print(i)
    
plt.figure()
plt.plot(xs, ys)
plt.xlabel("gamma")
plt.ylabel("GICtest")
plt.show()

#### Graficando el GIC
# GICtest grafica para ir buscando un buen inicio para la minimizacion
xs = np.linspace(0.1, 30, 8)
ys = np.zeros_like(xs)

for i in range(len(xs)):
    ys[i] = GICtrain(xs[i])
    print(i)
    
plt.figure()
plt.plot(xs, ys)
plt.xlabel("gamma")
plt.ylabel("GICtrain")
plt.show()

E_test(41)

E_test(350)

###### OPTIMIZANDO
start_th_calculus = time.time()
solu=sp.optimize.minimize( GICtest,145)
end_th_calculus = time.time() 

end_th_calculus-start_th_calculus

sp.optimize.minimize( GICtest,145)

########################## Generalized Cross-Validation  ####### some future work
I_T=np.identity(T)
I_h_T=np.identity(h_T)

def A_train(gamma):
  return np.transpose(SIGMA)@sp.linalg.inv( SIGMA@np.transpose(SIGMA)+T*gamma*I_n)@SIGMA

def A_test(gamma):
  return np.transpose(h_SIGMA)@sp.linalg.inv( h_SIGMA@np.transpose(h_SIGMA)+T*gamma*I_n)@h_SIGMA


  
def GCV_1train(gamma):
  return pow(T,-1)*np.transpose((I_T-A_train(gamma))@(np.transpose(Y_train)))@((I_T-A_train(gamma))@(np.transpose(Y_train)))
  
def GCV_2train(gamma):
  return pow(pow(T,-1)*np.trace(I_T-A_train(gamma)),2)


def GCV_1test(gamma):
  return pow(h_T,-1)*np.transpose((I_h_T-A_test(gamma))@(np.transpose(Y_test)))@((I_h_T-A_test(gamma))@(np.transpose(Y_test)))
  
def GCV_2test(gamma):
  return pow(pow(h_T,-1)*np.trace(I_h_T-A_test(gamma)),2)



def GCV_train(gamma):
  return GCV_1train(gamma)/GCV_2train(gamma)

def GCV_test(gamma):
  return GCV_1test(gamma)/GCV_2test(gamma)

#### test
xs = np.linspace(75, 200, 15)
ys = np.zeros_like(xs)

for i in range(len(xs)):
    ys[i] = GCV_test(xs[i])
    
plt.figure()
plt.plot(xs, ys)
plt.xlabel("gamma")
plt.ylabel("GCV_test")
plt.show()




#### train
xs = np.linspace(75, 200, 15)
ys = np.zeros_like(xs)

for i in range(len(xs)):
    ys[i] = GCV_train(xs[i])
    
plt.figure()
plt.plot(xs, ys)
plt.xlabel("gamma")
plt.ylabel("GCV_train")
plt.show()

minimize(GCV_train,120.0)

minimize(GCV_test,120.0)

def mu_train1(gamma):
  return (1/T)*(np.trace(A_train(gamma)))

def mu_test1(gamma):
  return (1/h_T)*(np.trace(A_test(gamma)))

def mu_train2(gamma):
  return (1/T)*(np.trace(A_train(gamma)@A_train(gamma)  ))

def mu_test2(gamma):
  return (1/h_T)*(np.trace(A_test(gamma)@A_test(gamma)  ))


gamma=120.0

(mu_train1(gamma)**2.0)/mu_train2(gamma)

mu_test1(gamma)**2.0/mu_test2(gamma)

mu_train1(gamma)
mu1=mu_train1(gamma)

mu_train2(gamma)
mu2=mu_train2(gamma)

(2*mu1+(mu1**2)/(mu2))*(1/(1-mu1)**2)
