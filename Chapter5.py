# -*- coding: utf-8 -*-
"""MNN

Automatically generated by Colaboratory.

"""

from keras.layers import Activation
from keras import backend as K
from keras.utils.generic_utils import get_custom_objects
from keras import initializers
import numpy as np
import matplotlib.pyplot as plt
import keras

import math

### usando GPU
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

"""Datasets"""

################################333333#######    MNIST
  from keras.datasets import mnist

  # lectura de los datos
  (train_images, train_labels), (test_images, test_labels) = mnist.load_data()

  # prepocesamiento de los datos
  train_images = train_images.reshape((60000, 28 * 28))
  train_images = train_images.astype('float32') / 255

  test_images = test_images.reshape((10000, 28 * 28))
  test_images = test_images.astype('float32') / 255

  numIm, szIm = train_images.shape

  from keras.utils import to_categorical
  train_labels = to_categorical(train_labels)
  test_labels  = to_categorical(test_labels)

#######3#####################################333 FASHION MNIST
from keras.datasets import fashion_mnist

  # lectura de los datos
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

  # prepocesamiento de los datos
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype('float32') / 255

test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255

numIm, szIm = train_images.shape

from keras.utils import to_categorical
train_labels = to_categorical(train_labels)
test_labels  = to_categorical(test_labels)

# Arquitectura de la red
  from keras import models
  from keras import layers
  import time

  def function31(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(784)
  
  def function32(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(200)
  
  def function33(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(300)
  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=200,activation=function31 ,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=300,activation=function32 ,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=250,activation=function33,use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation=function3,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  tic=time.time()
  history = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 10, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=1)#,
                   #callbacks=[time_callback])
  #nuevas5[i] = time.time()-a  #sum(time_callback.times)
  y_pred = nn.predict(test_images).squeeze()
  score = nn.evaluate(test_images, test_labels, verbose=0)
  #nuevas5_acc[i]=score[1]
  print('Tiempo: {} secs'.format(time.time()-tic))
  # summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

# Arquitectura de la red
  from keras import models
  from keras import layers
  import time


  ### FUNCIÓN DE ACTIVACIÓN NUEVA
  def function11(x):
    a=0.605706
    b=1.0/np.sqrt(0.0274153)
    return ( (K.tanh(x)-a*x)*b )/np.sqrt(784)

  def function12(x):
    a=0.605706
    b=1.0/np.sqrt(0.0274153)
    return ( (K.tanh(x)-a*x)*b )/np.sqrt(250)
  
  def function13(x):
    a=0.605706
    b=1.0/np.sqrt(0.0274153)
    return ( (K.tanh(x)-a*x)*b )/np.sqrt(350)

  def function14(x):
    a=0.605706
    b=1.0/np.sqrt(0.0274153)
    return ( (K.tanh(x)-a*x)*b )/np.sqrt(150)

  def function15(x):
    a=0.605706
    b=1.0/np.sqrt(0.0274153)
    return ( (K.tanh(x)-a*x)*b )/np.sqrt(100)  

  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=250,activation=function11, input_shape=(szIm,), kernel_initializer=initializers.random_normal(0,1), use_bias=False))
  nn.add(layers.Dense(units=350,activation=function12, kernel_initializer=initializers.glorot_normal(), use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=150,activation=function13, kernel_initializer=initializers.glorot_normal(),use_bias=False )) # , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=100,activation=function14, kernel_initializer=initializers.glorot_normal(), use_bias=False )) # , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=150,activation=function15, kernel_initializer=initializers.glorot_normal(), use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])


  history0 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,

  y_pred = nn.predict(test_images).squeeze()
  score = nn.evaluate(test_images, test_labels, verbose=0)

  
  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=250,activation='tanh', input_shape=(szIm,), kernel_initializer=initializers.glorot_normal(),use_bias=False))
  nn.add(layers.Dense(units=350,activation='tanh', kernel_initializer=initializers.glorot_normal(), use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=150,activation='tanh', kernel_initializer=initializers.glorot_normal(), use_bias=False )) # , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=100,activation='tanh', kernel_initializer=initializers.glorot_normal(), use_bias=False )) # , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=150,activation='tanh', kernel_initializer=initializers.glorot_normal(), use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])


  history2 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,

  y_pred = nn.predict(test_images).squeeze()
  score = nn.evaluate(test_images, test_labels, verbose=0)

plt.plot(history2.history['acc'],'b--')
plt.plot(history0.history['acc'],'r--')
#plt.plot(history0.history['acc'],'g--')
plt.plot(history2.history['val_acc'],'b')
plt.plot(history0.history['val_acc'],'r')
#plt.plot(history0.history['val_acc'],'g')
plt.title('tanh vs modified tanh')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train_classic','train_mod','val_classic','val_mod'], loc='lower right')
plt.show()

np.max(history0.history['val_acc'])

np.max(history2.history['val_acc'])

###### metrics de la tanh
argm_tanh=[0]*30
argm_tanh_c=[0]*30
max_tanh=[0]*30
max_tanh_c=[0]*30
for i in range(30):
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time


  ### FUNCIÓN DE ACTIVACIÓN NUEVA
  def function11(x):
    a=0.605706
    b=1.0/np.sqrt(0.0274153)
    return ( (K.tanh(x)-a*x)*b )/np.sqrt(784)

  def function12(x):
    a=0.605706
    b=1.0/np.sqrt(0.0274153)
    return ( (K.tanh(x)-a*x)*b )/np.sqrt(250)
  
  def function13(x):
    a=0.605706
    b=1.0/np.sqrt(0.0274153)
    return ( (K.tanh(x)-a*x)*b )/np.sqrt(350)

  def function14(x):
    a=0.605706
    b=1.0/np.sqrt(0.0274153)
    return ( (K.tanh(x)-a*x)*b )/np.sqrt(150)

  def function15(x):
    a=0.605706
    b=1.0/np.sqrt(0.0274153)
    return ( (K.tanh(x)-a*x)*b )/np.sqrt(100)  
  

  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=250,activation=function11,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0),use_bias=False))
  nn.add(layers.Dense(units=350,activation=function12,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=150,activation=function13,use_bias=False )) # , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=100,activation=function14,use_bias=False )) # , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=150,activation=function15,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history1 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,
                   #callbacks=[time_callback])


  argm_tanh[i]=np.argmax(history1.history['val_acc'])

  max_tanh[i]=np.max(history1.history['val_acc'])

  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=250,activation='tanh',input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0),use_bias=False))
  nn.add(layers.Dense(units=350,activation='tanh',use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=150,activation='tanh',use_bias=False )) # , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=100,activation='tanh',use_bias=False )) # , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=150,activation='tanh',use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history2 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,
                   #callbacks=[time_callback])

  argm_tanh_c[i]=np.argmax(history2.history['val_acc'])

  max_tanh_c[i]=np.max(history2.history['val_acc'])

print('the argmax for tanh modified', sum(argm_tanh)/30)
print('the max for tanh modified', sum(max_tanh)/30)
print('the argmax for tanh', sum(argm_tanh_c)/30)
print('the max for tanh', sum(max_tanh_c)/30)

###### funciones nuevas 
#nuevas5=[0]*20
#nuevas5_acc=[0]*20
#for i in range(20):
  
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time

  
  def function31(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(784)
  
  def function32(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(200)
  
  def function33(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(300)
  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=200,activation=function31 ,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0),use_bias=False))
  nn.add(layers.Dense(units=300,activation=function32 ,kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0),use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=250,activation=function33,kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0),use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation=function3,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])


  history3 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,

  
  
##### funciones normales  

  from keras import models
  from keras import layers
  import time
  
  def seno(x):
    return K.sin(x)

  nn = models.Sequential()  
  nn.add(layers.Dense(units=200,activation=seno,input_shape=(szIm,),kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0),use_bias=False))
  nn.add(layers.Dense(units=300,activation=seno,kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0),use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=250,activation=seno,kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0),use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation='linear',use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=10,activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])


  history4 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0) # ,

plt.plot(history4.history['acc'],'b--')
plt.plot(history3.history['acc'],'r--')
plt.plot(history4.history['val_acc'],'b')
plt.plot(history3.history['val_acc'],'r')
plt.title('sin vs sin modified')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train_classic','train_mod','val_classic','val_mod'], loc='lower right')
plt.show()

#                                   
argm_sin=[0]*30
argm_sin_c=[0]*30
max_sin=[0]*30
max_sin_c=[0]*30

for i in range(30):  
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time

  
  def function31(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(784)
  
  def function32(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(200)
  
  def function33(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(300)
  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=200,activation=function31 ,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0),use_bias=False))
  nn.add(layers.Dense(units=300,activation=function32 ,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=250,activation=function33,use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation=function3,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history3 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,
                   #callbacks=[time_callback])
  argm_sin[i]=np.argmax(history3.history['val_acc'])

  max_sin[i]=np.max(history3.history['val_acc'])
  
  
##### funciones normales  

  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time
  
  def seno(x):
    return K.sin(x)

  nn = models.Sequential()  
  nn.add(layers.Dense(units=200,activation=seno,input_shape=(szIm,),kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0),use_bias=False))
  nn.add(layers.Dense(units=300,activation=seno,use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=250,activation=seno,use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation='linear',use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=10,activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history4 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0) # ,
                   #callbacks=[time_callback])

  argm_sin_c[i]=np.argmax(history4.history['val_acc'])

  max_sin_c[i]=np.max(history4.history['val_acc'])

print('the argmax for sin modified', sum(argm_sin)/30)
print('the max for sin modified', sum(max_sin)/30)
print('the argmax for sin', sum(argm_sin_c)/30)
print('the max for sin', sum(max_sin_c)/30)

###### funciones nuevas
#nuevas3=[0]*20
#nuevas3_acc=[0]*20
#for i in range(20):
  
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time


  ### FUNCIÓN DE ACTIVACIÓN NUEVA
  def function11(x):
    a=0.206621
    b=1.0/2.0
    c=1.0/np.sqrt(0.000686813)
    return ( (K.sigmoid(x)-a*x-b)*c )/np.sqrt(784)

  def function12(x):
    a=0.206621
    b=1.0/2.0
    c=1.0/np.sqrt(0.000686813)
    return ( (K.sigmoid(x)-a*x-b)*c )/np.sqrt(200)
  
  def function13(x):
    a=0.206621
    b=1.0/2.0
    c=1.0/np.sqrt(0.000686813)
    return ( (K.sigmoid(x)-a*x-b)*c )/np.sqrt(350)
  
  def function14(x):
    a=0.206621
    b=1.0/2.0
    c=1.0/np.sqrt(0.000686813)
    return ( (K.sigmoid(x)-a*x-b)*c )/np.sqrt(250)
  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=200,activation=function11,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation=function12,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=250,activation=function13,use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation=function14,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history5 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,
                   #callbacks=[time_callback])

  
  
  ##### funciones normales  
#normales3=[0]*20
#normales3_acc=[0]*20
#for i in range(20):
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time
  

  nn = models.Sequential()  
  nn.add(layers.Dense(units=200,activation='sigmoid',input_shape=(szIm,),kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation='sigmoid', use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=250,activation='sigmoid', use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation='sigmoid',use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=10,activation='softmax', use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history6 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0) # ,
                   #callbacks=[time_callback])

plt.plot(history6.history['acc'],'b--')
plt.plot(history5.history['acc'],'r--')
plt.plot(history6.history['val_acc'],'b')
plt.plot(history5.history['val_acc'],'r')
plt.title('sigmoid vs sigmoid modified')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train_classic','train_mod','val_classic','val_mod'], loc='lower right')
plt.show()

#                                      
argm_sigmoid=[0]*30
argm_sigmoid_c=[0]*30
max_sigmoid=[0]*30
max_sigmoid_c=[0]*30

for i in range(30):
###### funciones nuevas

  
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time


  ### FUNCIÓN DE ACTIVACIÓN NUEVA
  def function11(x):
    a=0.206621
    b=1.0/2.0
    c=1.0/np.sqrt(0.000686813)
    return ( (K.sigmoid(x)-a*x-b)*c )/np.sqrt(784)

  def function12(x):
    a=0.206621
    b=1.0/2.0
    c=1.0/np.sqrt(0.000686813)
    return ( (K.sigmoid(x)-a*x-b)*c )/np.sqrt(250)
  
  def function13(x):
    a=0.206621
    b=1.0/2.0
    c=1.0/np.sqrt(0.000686813)
    return ( (K.sigmoid(x)-a*x-b)*c )/np.sqrt(350)
  
  def function14(x):
    a=0.206621
    b=1.0/2.0
    c=1.0/np.sqrt(0.000686813)
    return ( (K.sigmoid(x)-a*x-b)*c )/np.sqrt(150)
  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=250,activation=function11,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation=function12,use_bias=False )) # , input_shape=(szIm,)))  
#  nn.add(layers.Dense(units=150,activation=function13,use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation=function14,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])


  history5 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,
                   #callbacks=[time_callback])
    
  argm_sigmoid[i]=np.argmax(history5.history['val_acc'])

  max_sigmoid[i]=np.max(history5.history['val_acc'])
  
  
  ##### funciones normales  

  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time
  

  nn = models.Sequential()  
  nn.add(layers.Dense(units=250,activation='sigmoid',input_shape=(szIm,),kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation='sigmoid', use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation='sigmoid', use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation='sigmoid',use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=10,activation='softmax', use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])


  history6 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0) # ,
                   #callbacks=[time_callback])

  argm_sigmoid_c[i]=np.argmax(history6.history['val_acc'])

  max_sigmoid_c[i]=np.max(history6.history['val_acc'])
  
  print(i)

print('the argmax for sigmoid modified', sum(argm_sigmoid)/30)
print('the max for sigmoid modified', sum(max_sigmoid)/30)
print('the argmax for sigmoid', sum(argm_sigmoid_c)/30)
print('the max for sigmoid', sum(max_sigmoid_c)/30)

##### PRUEBA CON LA SOFTPLUS
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time


  ### FUNCIÓN DE ACTIVACIÓN NUEVA
  def softplus32(x):
    a=1.0/2.0
    b=0.80605
    c=1.0/np.sqrt(0.0215442)
    return ( c*(K.softplus(x)-a*x-b) )/np.sqrt(784)
  
  def softplus33(x):
    a=1.0/2.0
    b=0.80605
    c=1.0/np.sqrt(0.0215442)
    return ( c*(K.softplus(x)-a*x-b) )/np.sqrt(250)

  def softplus34(x):
    a=1.0/2.0
    b=0.80605
    c=1.0/np.sqrt(0.0215442)
    return ( c*(K.softplus(x)-a*x-b) )/np.sqrt(350)

  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=250,activation=softplus32,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation=softplus33,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=150,activation=softplus34,use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation=function14,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])


  history_1 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,

  
  
  ##### funciones normales  

  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time
  

  nn = models.Sequential()  
  nn.add(layers.Dense(units=250,activation='softplus',input_shape=(szIm,),kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation='softplus', use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=150,activation='softplus', use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation='sigmoid',use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=10,activation='softmax', use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])


  history_2 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0) # ,

plt.plot(history_2.history['acc'],'b--')
plt.plot(history_1.history['acc'],'r--')
plt.plot(history_2.history['val_acc'],'b')
plt.plot(history_1.history['val_acc'],'r')
plt.title('softplus vs softplus modified')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train_classic','train','val_classic','val'], loc='lower right')
plt.show()

np.argmax(history_2.history['val_acc'])

np.argmax(history_1.history['val_acc'])

# Arquitectura de la red
  from keras import models
  from keras import layers
  import time


  ### FUNCIÓN DE ACTIVACIÓN NUEVA
  
  def function31(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(784)
  
  def function32(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(250)
  
  def function33(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(350)

  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=250,activation=function31,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation=function32,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=150,activation=function33,use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation=function14,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])


  history_3 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,

  
  
  ##### funciones normales  

  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time
  


  nn = models.Sequential()  
  nn.add(layers.Dense(units=250,activation='relu',input_shape=(szIm,),kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation='relu', use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=150,activation='relu', use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation='sigmoid',use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=10,activation='softmax', use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history_4 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0) # ,

plt.plot(history_4.history['acc'],'b--')
plt.plot(history_3.history['acc'],'r--')
plt.plot(history_4.history['val_acc'],'b')
plt.plot(history_3.history['val_acc'],'r')
plt.title('relu vs sin modified')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train_relu','train_sin_mod','val_relu','val_sin_mod'], loc='lower right')
plt.show()

np.max(history_4.history['val_acc'])

np.max(history_3.history['val_acc'])

def sigmoid(x):
  return pow((1+np.exp(-x)),-1.0)

def function11(x):
  a=0.206621
  b=1.0/2.0
  c=1.0/np.sqrt(0.000686813)
  return ( (sigmoid(x)-a*x-b)*c )
  
def function21(x):
  a=0.605706
  b=1.0/np.sqrt(0.0274153)
  return ( (np.tanh(x)-a*x)*b )
  
def function31(x):
  a=0.606531
  c=1.0/np.sqrt(0.0644529)
  return ( c*(np.sin(x)-a*x) )

def cubo(x):
  return -(1.0/np.sqrt(6.0))*(x**3-3*x)
  
t = np.arange(-10.0, 10.0, 0.1)

plt.plot(t, function11(t), 'r', t, function21(t), 'b', t, function31(t), 'g', t, t**2, 'y:', t, -t,'y--')
plt.title('activation functions')
plt.ylabel('F(x)')
plt.xlabel('x')
#plt.grid(True)
plt.axvline(0,linestyle='--')
plt.axhline(0,linestyle='--')
plt.legend(['sigmoid','tanh','sin'], loc='lower left')
plt.show()

t = np.arange(-2.5, 2.5, 0.1)
plt.plot(t, function11(t), 'r', t, function21(t), 'b', t, function31(t), 'g',t,t,'y')
plt.title('activation functions')
plt.ylabel('F(x)')
plt.xlabel('x')
#plt.grid(True)
plt.axvline(0,linestyle='--')
plt.axhline(0,linestyle='--')
plt.legend(['sigmoid','tanh','sin'], loc='upper right')
plt.show()



tt = np.arange(-2.0, 2.0, 0.1)

plt.plot( tt, function31(tt), 'r', tt, -(tt), 'b')
plt.title('activation functions')
plt.ylabel('F(x)')
plt.xlabel('x')
#plt.grid(True)
plt.axvline(0,linestyle='--')
plt.axhline(0,linestyle='--')
plt.legend(['sin', '-identity'], loc='upper right')
plt.show()



plt.plot( tt, -tt, 'r', tt, -tt**3, 'b')
plt.title('activation functions')
plt.ylabel('F(x)')
plt.xlabel('x')
#plt.grid(True)
plt.axvline(0,linestyle='--')
plt.axhline(0,linestyle='--')
plt.legend(['-identity','cube'], loc='upper right')
plt.show()

###### funciones nuevas             ##############################
#nuevas5=[0]*20
#nuevas5_acc=[0]*20
#for i in range(20):
  
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time

  
  def function31(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(784)
  
  def function32(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(200)
  
  def function33(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(300)
  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=200,activation=function31 ,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=300,activation=function32 ,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=250,activation=function33,use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation=function3,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history_u = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,
                   #callbacks=[time_callback])
  #nuevas5[i] = time.time()-a  #sum(time_callback.times)
  #y_pred = nn.predict(test_images).squeeze()
  #score = nn.evaluate(test_images, test_labels, verbose=0)
  #nuevas5_acc[i]=score[1]
  #print('Tiempo: {} secs'.format(time.time()-tic))
  
  
##### funciones normales  
#normales5=[0]*20
#normales5_acc=[0]*20
#for i in range(20):
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time
  
  def polinomio1(x):
    return -(1.0/np.sqrt(6.0))*(x**3-3*x)/np.sqrt(784)
  
  def polinomio2(x):
    return -(1.0/np.sqrt(6.0))*(x**3-3*x)/np.sqrt(200)
  
  def polinomio3(x):
    return -(1.0/np.sqrt(6.0))*(x**3-3*x)/np.sqrt(300)

  nn = models.Sequential()  
  nn.add(layers.Dense(units=200,activation=polinomio1,input_shape=(szIm,),kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),bias_initializer='zeros'))
  nn.add(layers.Dense(units=300,activation=polinomio2,use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=250,activation=polinomio3,use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation='linear',use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=10,activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history_uu = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0) # ,
                   #callbacks=[time_callback])
  #normales5[i] =time.time()-a   #sum(time_callback.times)
  #y_pred = nn.predict(test_images).squeeze()
  #score = nn.evaluate(test_images, test_labels, verbose=0)
  #normales5_acc[i]=score[1]
  #print('Tiempo: {} secs'.format(time.time()-tic))

plt.plot(history_uu.history['acc'],'b')
plt.plot(history_u.history['acc'],'r')
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train_polinomial','train_sin'], loc='upper left')
plt.show()

###### funciones nuevas             ##############################    CIFAR 10
#nuevas5=[0]*20
#nuevas5_acc=[0]*20
#for i in range(20):
  
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time

  
  def function31(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(784)
  
  def function32(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(200)
  
  def function33(x):
    a=0.606531
    c=1.0/np.sqrt(0.0644529)
    return ( c*(K.sin(x)-a*x) )/np.sqrt(300)
  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=200,activation=function31 ,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=300,activation=function32 ,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=250,activation=function33,use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation=function3,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history_u = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 20, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,
                   #callbacks=[time_callback])
  #nuevas5[i] = time.time()-a  #sum(time_callback.times)
  #y_pred = nn.predict(test_images).squeeze()
  #score = nn.evaluate(test_images, test_labels, verbose=0)
  #nuevas5_acc[i]=score[1]
  #print('Tiempo: {} secs'.format(time.time()-tic))

plt.plot(history_u.history['val_acc'],'b')
#plt.plot(history_u.history['val_acc'],'r')
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['test_sin'], loc='upper left')
plt.show()

######################################################



###### funciones nuevas
#nuevas3=[0]*20
#nuevas3_acc=[0]*20
#for i in range(20):
  
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time


  ### FUNCIÓN DE ACTIVACIÓN NUEVA
  def function11(x):
    return ( (np.sqrt(3.0*np.sqrt(5.0)/(3.0-np.sqrt(5) )))*(K.exp(-x**2)-1.0/np.sqrt(3.0) ) )/np.sqrt(784)

  def function12(x):
    return ( (np.sqrt(3.0*np.sqrt(5.0)/(3.0-np.sqrt(5) )))*(K.exp(-x**2)-1.0/np.sqrt(3.0) ) )/np.sqrt(250)
  

  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=250,activation=function11,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation=function12,use_bias=False )) # , input_shape=(szIm,)))  
#  nn.add(layers.Dense(units=150,activation=function13,use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation=function14,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history_5 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 25, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,
                   #callbacks=[time_callback])

  
  
  ##### funciones normales  
#normales3=[0]*20
#normales3_acc=[0]*20
#for i in range(20):
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time
  
  def gaussian(x):
    return K.exp(-x**2)

  nn = models.Sequential()  
  nn.add(layers.Dense(units=250,activation=gaussian,input_shape=(szIm,),kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation=gaussian, use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation='sigmoid', use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation='sigmoid',use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=10,activation='softmax', use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history_6 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 25, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0) # ,
                   #callbacks=[time_callback])

plt.plot(history_6.history['acc'],'b--')
plt.plot(history_5.history['acc'],'r--')
plt.plot(history_6.history['val_acc'],'b')
plt.plot(history_5.history['val_acc'],'r')
plt.title('Gaussian vs Gaussian modified')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train_classic','train_mod','val_classic','val_mod'], loc='lower right')
plt.show()

######################################################

#nuevas3=[0]*20
#nuevas3_acc=[0]*20
#for i in range(20):
  
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time


  ### FUNCIÓN DE ACTIVACIÓN NUEVA
  def function11(x):
    return ( (np.sqrt(1/0.826725))*( K.sin(2.0*x)+K.cos(3.0*x/2.0)-2*K.exp(-2.0)*x-K.exp(-9.0/8.0) ) )/np.sqrt(784)

  def function12(x):
    return ( (np.sqrt(1/0.826725))*( K.sin(2.0*x)+K.cos(3.0*x/2.0)-2*K.exp(-2.0)*x-K.exp(-9.0/8.0) ) )/np.sqrt(250)
  

  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=250,activation=function11,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation=function12,use_bias=False )) # , input_shape=(szIm,)))  
#  nn.add(layers.Dense(units=150,activation=function13,use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation=function14,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history7 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 10, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,
                   #callbacks=[time_callback])
  #nuevas3[i] = time.time()-a  #sum(time_callback.times)
  #y_pred = nn.predict(test_images).squeeze()
  #score = nn.evaluate(test_images, test_labels, verbose=0)
  #nuevas3_acc[i]=score[1]
  #print('Tiempo: {} secs'.format(time.time()-tic))
  
  
  ##### funciones normales  
#normales3=[0]*20
#normales3_acc=[0]*20
#for i in range(20):
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time
  
  def gaussian(x):
    return K.sin(x)

  nn = models.Sequential()  
  nn.add(layers.Dense(units=250,activation=gaussian,input_shape=(szIm,),kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation=gaussian, use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation='sigmoid', use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation='sigmoid',use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=10,activation='softmax', use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history8 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 10, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0) # ,
                   #callbacks=[time_callback])
  #normales3[i] =time.time()-a   #sum(time_callback.times)
  #y_pred = nn.predict(test_images).squeeze()
  #score = nn.evaluate(test_images, test_labels, verbose=0)
  #normales3_acc[i]=score[1]
  #print('Tiempo: {} secs'.format(time.time()-tic))

plt.plot(history8.history['acc'],'b')
plt.plot(history7.history['acc'],'r')
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train_classic','train'], loc='upper left')
plt.show()

######################################################



###### funciones nuevas
#nuevas3=[0]*20
#nuevas3_acc=[0]*20
#for i in range(20):
  
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time


  ### FUNCIÓN DE ACTIVACIÓN NUEVA
  def function11(x):
    return (( (1.0/np.sqrt(0.199788) ) ) *(K.cos(x)-1.0/np.sqrt(np.e) ) )/np.sqrt(784)

  def function12(x):
    return (( (1.0/np.sqrt(0.199788) ) ) *(K.cos(x)-1.0/np.sqrt(np.e) ) )/np.sqrt(250)
  

  
  nn = models.Sequential()   
  nn.add(layers.Dense(units=250,activation=function11,input_shape=(szIm,), kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation=function12,use_bias=False )) # , input_shape=(szIm,)))  
#  nn.add(layers.Dense(units=150,activation=function13,use_bias=False )) # , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation=function14,use_bias=False )) # , input_shape=(szIm,)))  
  nn.add(layers.Dense(units=10, activation='softmax',use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history11 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 40, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0)#,
                   #callbacks=[time_callback])

  
  ##### funciones normales  
#normales3=[0]*20
#normales3_acc=[0]*20
#for i in range(20):
  # Arquitectura de la red
  from keras import models
  from keras import layers
  import time
  
  def cose(x):
    return K.cos(x)

  nn = models.Sequential()  
  nn.add(layers.Dense(units=250,activation=cose,input_shape=(szIm,),kernel_initializer=initializers.random_normal(mean=0.0, stddev=1.0, seed=8),use_bias=False))
  nn.add(layers.Dense(units=350,activation=cose, use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=150,activation='sigmoid', use_bias=False))# , input_shape=(szIm,))) 
#  nn.add(layers.Dense(units=100,activation='sigmoid',use_bias=False))# , input_shape=(szIm,))) 
  nn.add(layers.Dense(units=10,activation='softmax', use_bias=False))

  nn.compile(optimizer='rmsprop',
             loss     ='categorical_crossentropy',
             metrics  =['accuracy'])

  #time_callback = TimeHistory()
  #a=time.time()
  history10 = nn.fit(x = train_images, 
                   y = train_labels, 
                   validation_split=0.2,
                   epochs    = 40, 
                   shuffle   = True,
                   #batch_size = 128,
                   verbose=0) # ,
                   #callbacks=[time_callback])

plt.plot(history10.history['acc'],'b--')
plt.plot(history11.history['acc'],'r--')
plt.plot(history10.history['val_acc'],'b')
plt.plot(history11.history['val_acc'],'r')
plt.title('cos vs cos modified')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train_classic','train_mod','val_classic','val_mod'], loc='lower right')
plt.show()
